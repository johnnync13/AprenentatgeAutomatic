{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3 - Train a REGRESSION Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "# Settings for the visualizations\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_linear_models\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is a simple technique that is useful for predicted problems.\n",
    "\n",
    "linear regression pros:\n",
    "* widely used\n",
    "+ runs fast\n",
    "+ easy to use (not a lot of tuning required)\n",
    "+ highly interpretable\n",
    "+ basis for many other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    "plt.plot(x,y,\"o\")\n",
    "plt.xlabel(\"$x_1 $\", fontsize=18)\n",
    "plt.ylabel(\"$y $\", rotation=90, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "$$\\textbf{y} = b_0+b_1 \\textbf{x}$$\n",
    "\n",
    "Ordinary Least Squares (OLS) is the simplest and most common **estimator** in which the two $b$'s are chosen to minimize the sum of squared distance between the predicted values and the actual values. \n",
    "\n",
    "Given the set of samples $(\\textbf{x},\\textbf{y})$, the objective is to minimize:\n",
    "\n",
    "$$ ||b_0 + b_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (b_0+b_1 x_{j} -  y_j )^2,$$ with respect to $b_0, b_1$.\n",
    "\n",
    "This expression is often called **sum of squared errors of prediction (SSE)**.\n",
    "\n",
    "## How to compute the OLS: Scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the use of zip in the next code:\n",
    "list(zip([2,3,4,5,6],[40,50,60,70,80]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "# Minimize the sum of squares using a lambda function\n",
    "\n",
    "sse = lambda b, x, y: np.sum((y - b[0] - b[1]*x) ** 2) # Store the sum of squared differences function\n",
    "# Lambda function is a small anonymous function. \n",
    "# It can take any number of arguments, but can only have one expression. \n",
    "# Syntax \"lambda arguments : expression\"\n",
    "\n",
    "b0,b1 = fmin(sse, [0,1], args=(x,y)); # Minimize the sum of squared differences\n",
    "# [0,1] is the initial guess for w[0] and w[1] in function sse.\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10], alpha=0.8) # Add the regression line, colored in blue\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], \"k:\") # Add pointed black line to illustrate the errors\n",
    "plt.xlim(2, 9); plt.ylim(0, 20) # Restrict the domain\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can minimize other criteria, such as the **sum of absolute differences between the predicted values and the actual values**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabs = lambda b, x, y: np.sum(np.abs(y - b[0] - b[1]*x)) # Lambda function \n",
    "\n",
    "b0,b1 = fmin(sabs, [0,1], args=(x,y)) # Minimize the sum of absolute differences\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10]) # Add the regression line, colored in blue\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], \"k:\") # Add pointed black line to illustrate the errors\n",
    "plt.xlim(2, 9); plt.ylim(0, 20) # Restrict the domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Adversiting dataset\n",
    "\n",
    "Let's play with an Adversiting dataset from the book \"Introduction to Statistical Learning\"\n",
    "\n",
    "**The Data** \n",
    "\n",
    "A data frame with 200 observations on the following 4 variables (TV ,Radio, Newspaper, Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is a popular approach for several reasons. \n",
    "\n",
    "+ It is computationally cheap to calculate the coefficients. \n",
    "+ It is easier to interpret than more sophisticated models. In situations where the goal is understanding a simple model in detail, rather than estimating the response well, they can provide insight into what the model captures. \n",
    "+ Finally, in situations where there is a lot of noise, it may be hard to find the true functional form, so a constrained model can perform quite well compared to a complex model which is more affected by noise.\n",
    "\n",
    "The resulting model is represented as follows:\n",
    "\n",
    "$$\\widehat{\\textbf{y}} = \\widehat{b}_0+\\widehat{b}_1 \\textbf{x}$$\n",
    "\n",
    "Here the hats on the variables represent the fact that they are estimated from the data we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/Advertising.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the features?\n",
    "\n",
    "* TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "* Radio: advertising dollars spent on Radio\n",
    "* Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the response?\n",
    "\n",
    "* Sales: sales of a single product in a given market (in thousands of items)\n",
    "\n",
    "What else do we know?\n",
    "\n",
    "Because the response variable is continuous, this is a regression problem.\n",
    "There are 200 observations (represented by the rows), and each observation is a single market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['TV']]\n",
    "y = df[['Sales']]\n",
    "plt.plot(X,y,\"o\", alpha=0.3)\n",
    "plt.xlabel(\"$x_1 : TV$\", fontsize=18)\n",
    "plt.ylabel(\"$y : Sales$\", rotation=90, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((200, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = np.array([[0], [300]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the book actually corresponds to the following code, with a legend and axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"o\",alpha=0.3)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in this case, far values are penalized less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Linear Regression using Statsmodel package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "model = smf.ols(formula='Sales ~ TV', data=df)\n",
    "model = model.fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X)\n",
    "plt.scatter(X, y, alpha=0.3, color='orchid')\n",
    "plt.plot(X, y_pred, '-', color='darkorchid', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Linear Regression using SciKitLearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "y_pred = lin_reg.predict(X)\n",
    "# plot the output\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X, y, alpha=0.3, color='orchid')\n",
    "plt.plot(X, y_pred, '-', color='darkorchid', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lin_reg.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.3f'\n",
    "      % mean_squared_error(y, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination (R^2): %.3f'\n",
    "      % r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "Simple linear regression can easily be extended to include multiple features. This is called multiple linear regression:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1X_1 + ... \\beta_pX_p + \\epsilon $$\n",
    "\n",
    "We interpret $\\beta_j$ as the average effect on $Y$ of a one unit increase in $X_j$ , holding all other predictors fixed. In the advertising example, the model becomes:\n",
    "\n",
    "$$ {\\color{red}{sales}} = \\beta_0 + \\beta_1 ~x~ {\\color{red}{TV}} + \\beta_2 ~x~ {\\color{red}{radio}} + \\beta_3 ~x~ {\\color{red}{newspaper}} + \\epsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.pairplot(df, x_vars=['TV','Radio','Newspaper'],y_vars='Sales', height=7, aspect=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with statsmodel\n",
    "model = smf.ols(formula='Sales ~ TV + Radio', data=df)\n",
    "model = model.fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TV','Radio','Newspaper']\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(df[features], y)\n",
    "\n",
    "y_pred = lin_reg.predict(df[features])\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lin_reg.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.3f'\n",
    "      % mean_squared_error(y, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination (R^2): %.3f'\n",
    "      % r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the Coefficient of determination (R^2) has improved from 0.612 to 0.897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()\n",
    "print (corrMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  Macroeconomic dataset\n",
    "\n",
    "To start with we load the Longley dataset of US macroeconomic data from the R datasets website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read data\n",
    "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df.columns = ['GNPdeflator', 'GNP', 'Unemployed', 'ArmedForces', 'Population','Year', 'Employed']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macroeconomic data from 1947 to 1962.\n",
    "\n",
    "We will use the variable Total Derived Employment ('Employed') as our response $\\textbf{y}$ and Gross National Product ('GNP') as our predictor $\\textbf{x}$.\n",
    "\n",
    "We also add a constant term so that we fit the intercept of our linear model: $X=(\\textbf{1},\\textbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.pairplot(df, x_vars=['GNPdeflator','Unemployed','ArmedForces','Population','Year','Employed'],y_vars='GNP', height=7, aspect=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another interesting graph from seaborn\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with sciklearn\n",
    "features = ['GNPdeflator','Unemployed','ArmedForces','Population','Year','Employed']\n",
    "target = ['GNP']\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lin_reg.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.3f'\n",
    "      % mean_squared_error(y, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination (R^2): %.3f'\n",
    "      % r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with statsmodel\n",
    "model = smf.ols(formula='GNP ~ GNPdeflator + Unemployed + ArmedForces + Population + Year + Employed', data=df)\n",
    "model = model.fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see signs of non linearity in the data which has not been captured by the model. \n",
    "\n",
    "In order to capture this non-linear effects, we have another type of regression known as polynomial regression. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Models\n",
    "\n",
    "### Ridge Regression.\n",
    "Ridge Regression penalizes the coefficients if they are too far from zero, thus enforcing them to be small in a continuous way. This way, it decreases model complexity while keeping all variables in the model.\n",
    "\n",
    "For that, Ridge regression adds a **$\\ell_2$-norm** regularization term to the sum of squared errors of prediction (SSE). Given the set of samples  (𝑋,𝐲) , the objetive is to minimize:\n",
    "\n",
    "$$ minimize(\\sum_{i=0}^n (y_i - B_0- \\sum_{j=1}^pB_jx_{ij})^2 - \\lambda\\sum_{j=1}^pB_j^2) $$\n",
    "\n",
    "### Lasso Regression:\n",
    "\n",
    "Often, in real problems, there are uninformative variables in the data which prevent proper modeling of the problem and thus, the building of a correct regression model. In such cases, a feature selection process is crucial to select only the informative features and discard non-informative ones. This can be achieved by sparse methods which use a penalization approach, such as **Lasso** (least absolute shrinkage and selection operator) to set some model coefficients to zero (thereby discarding those variables). Sparsity can be seen as an application of Occam’s razor: prefer simpler models to complex ones.\n",
    "For that, Lasso regression adds a **$\\ell_1$-norm** regularization term to the sum of squared errors of prediction (SSE).  Given the set of samples  (𝑋,𝐲) , the objetive is to minimize:\n",
    "\n",
    "$$ minimize(\\sum_{i=0}^n (y_i - B_0- \\sum_{j=1}^pB_jx_{ij})^2 - \\lambda\\sum_{j=1}^p|B_j|)$$\n",
    "\n",
    "#### Geometric explanantion:\n",
    "The left panel shows L1 regularization (lasso regularization) and the right panel L2 regularization (Ridge regression). The ellipses indicate the distribution for no regularization. The blue lines show the constraints due to regularization (limiting  𝜃2  for ridge regression and  |𝜃|  for Lasso regression). The corners of the L1 regularization create more opportunities for the solution to have zeros for some of the weights.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*Jd03Hyt2bpEv1r7UijLlpg.png \"Regularization\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression\n",
    "regr_ridge = linear_model.Ridge(alpha=.3) # Create a Ridge regressor\n",
    "regr_ridge.fit(X, y)  # Perform the fitting\n",
    "\n",
    "print('Coeff and intercept: {} {}'.format(regr_ridge.coef_,  regr_ridge.intercept_))\n",
    "coef = pd.Series(np.abs(regr_ridge.coef_[0]),features).sort_values()\n",
    "coef.plot(kind='bar', title='Ridge Coefficients',ylabel=\"|$b_j$|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso Regression\n",
    "regr_lasso = linear_model.Lasso(alpha=.3,tol=0.001) # Create a Ridge regressor\n",
    "regr_lasso.fit(X, y)  # Perform the fitting\n",
    "\n",
    "print('Coeff and intercept: {} {}'.format(regr_lasso.coef_,  regr_lasso.intercept_))\n",
    "\n",
    "coef = pd.Series(np.abs(regr_lasso.coef_),features).sort_values()\n",
    "coef.plot(kind='bar', title='Lasso Coefficients',ylabel=\"|$b_j$|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-2, 5, n_alphas)\n",
    "\n",
    "coefs_ridge = []\n",
    "\n",
    "for l in alphas:\n",
    "    regr_ridge = linear_model.Ridge(alpha=l) # Create a Ridge regressor\n",
    "    regr_ridge.fit(X, y)  # Perform the fitting\n",
    "    coefs_ridge.append(regr_ridge.coef_[0])\n",
    "    \n",
    "coefs_lasso = []\n",
    "for l in alphas:\n",
    "    regr_lasso = linear_model.Lasso(alpha=l,tol =0.001) # Create a Ridge regressor\n",
    "    regr_lasso.fit(X, y)  # Perform the fitting\n",
    "    coefs_lasso.append(regr_lasso.coef_)\n",
    "# #############################################################################\n",
    "# Display results\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6), sharey=True)\n",
    "\n",
    "\n",
    "axs[0].plot(alphas, coefs_ridge)\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_title('Ridge coefficients as a function of the regularization')\n",
    "axs[0].axis('tight')\n",
    "axs[0].set_xlabel('alpha')\n",
    "axs[0].set_ylabel('weights')\n",
    "\n",
    "axs[1].plot(alphas, coefs_lasso)\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_title('Lasso coefficients as a function of the regularization')\n",
    "axs[1].axis('tight')\n",
    "axs[1].set_xlabel('alpha')\n",
    "axs[1].set_ylabel('weights')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE: Create and evalualuate a regression model for **THIS** dataset\n",
    "\n",
    "Remember the ML pipeline: \n",
    "\n",
    "#### ML Pipeline\n",
    "* Setting up the environment and data import\n",
    "* Understanding the data\n",
    "* Exploratory Data Analysis\n",
    "* Linear Regression Model\n",
    "* Preparation and splitting the data\n",
    "* Train and Test the Model\n",
    "* Train and Test New Model\n",
    "* Compare the models\n",
    "* Model Performance\n",
    "* Applying on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 - Setting up the enviroment and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ssegui/ml_ub/master/notebooks/dataset/regression_healthcare/datasets_13720_18513_insurance.csv')\n",
    "df['charges'] = np.log(df['charges'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE: Insurance contractor age, year. \n",
    "#       min age : 18\n",
    "#       max age : 64\n",
    "\n",
    "sns.distplot(df.age,bins=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEX: Insurance contractor gender, [female, male ]\n",
    "sns.countplot(data=df, y = 'sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI: Body mass index, providing an understanding of body, weights that are relatively \n",
    "# high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of \n",
    "# height to weight, ideally 18.5 to 24.9\n",
    "sns.distplot(df.bmi,bins=65)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smoker: smoking, [yes, no]\n",
    "sns.countplot(data=df,y='smoker')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"smoker\", kind=\"count\",hue = 'sex', palette=\"pink\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Children: number of children covered by health insurance / Number of dependents\n",
    "sns.countplot(data=df,y='children')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Children: number of children covered by health insurance / Number of dependents\n",
    "sns.countplot(data=df,y='region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charges: Individual medical costs billed by health insurance, $ #predicted value\n",
    "sns.distplot(df.charges)\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x=df[\"charges\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for Machine Learning algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "cat_attribs = ['sex','region','smoker']\n",
    "num_attribs = ['bmi','age','children']\n",
    "\n",
    "X_num = df[num_attribs]\n",
    "X_cat = df[cat_attribs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "X_train = full_pipeline.fit_transform(train_set)\n",
    "y_train = train_set['charges']\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "tmp = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train.values)], axis=1)\n",
    "tmp.columns = ['bmi','age','children','sex_0','sex_1','region_0','region_1','region_2','region_3','smoker_0','smoker_1','charges']\n",
    "\n",
    "corrMatrix = tmp.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "sns.heatmap(corrMatrix, vmax=.8, square=True, annot=True,ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "some_data = train_set.iloc[:5]\n",
    "some_labels = train_set[['charges']].iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predicted Charges:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "y_pred = lin_reg.predict(X_train)\n",
    "lin_mse = mean_squared_error(y_train, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_mae = mean_absolute_error(y_train, y_pred)\n",
    "\n",
    "print(lin_rmse,lin_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def cross_val_evaluation(model,X_train,y_train,model_name):\n",
    "    scores = cross_val_score(model, X_train, y_train,cv=5)\n",
    "    print(\"\\n \",model_name)\n",
    "    display_scores(scores)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "cross_val_evaluation(lin_reg,X_train,y_train,'Linear Regression')\n",
    "\n",
    "ridge_reg = linear_model.Ridge(alpha=.3) # Create a Ridge regressor\n",
    "cross_val_evaluation(ridge_reg,X_train,y_train,'Ridge Regression')\n",
    "\n",
    "lasso_reg = linear_model.Lasso(alpha=0.01) # Create a Ridge regressor\n",
    "cross_val_evaluation(lasso_reg,X_train,y_train,'Lasso Regression')\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "cross_val_evaluation(forest_reg,X_train,y_train,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'alpha': np.logspace(-2, 5, 100)}]\n",
    "\n",
    "ridge_reg = linear_model.Ridge() # Create a Ridge regressor\n",
    "grid_search_ridge = GridSearchCV(ridge_reg, param_grid, cv=5,scoring=\"neg_mean_squared_error\",\n",
    "                                 return_train_score=True, n_jobs=-1)\n",
    "\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "print(grid_search_ridge.best_params_)\n",
    "print(grid_search_ridge.best_score_)\n",
    "pd.DataFrame(grid_search_ridge.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = linear_model.Lasso(tol=0.01) # Create a Ridge regressor\n",
    "grid_search_lasso = GridSearchCV(lasso_reg, param_grid, cv=5,scoring=\"neg_root_mean_squared_error\",\n",
    "                                 return_train_score=True,n_jobs=-1)\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "print(grid_search_lasso.best_params_)\n",
    "print(grid_search_lasso.best_score_)\n",
    "\n",
    "pd.DataFrame(grid_search_lasso.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range= np.logspace(-2, 2, 20)\n",
    "\n",
    "train_scores, valid_scores = validation_curve(ridge_reg, X_train, y_train, param_name= \"alpha\",\n",
    "                                              param_range= param_range,\n",
    "                                              cv=3)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.title(\"Validation Curve with Ridge Regression\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same with lasso model\n",
    "param_range= np.logspace(-2, 3, 20)\n",
    "\n",
    "train_scores, valid_scores = validation_curve(lasso_reg, X_train, y_train, param_name= \"alpha\",\n",
    "                                              param_range= param_range,\n",
    "                                              cv=3)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.title(\"Validation Curve with Ridge Regression\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Linearity\n",
    "X_test = full_pipeline.transform(test_set)\n",
    "y_test = test_set['charges']\n",
    "\n",
    "lin_reg.fit(X_train,y_train)\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.scatterplot(y_test,y_pred,color='g')\n",
    "plt.title('Check for Linearity:\\n Actual Vs Predicted value')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Real')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Linearity --  LASSSO\n",
    "X_test = full_pipeline.transform(test_set)\n",
    "y_test = test_set['charges']\n",
    "\n",
    "grid_search_lasso.fit(X_train,y_train)\n",
    "y_pred = grid_search_lasso.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.scatterplot(y_test,y_pred,color='g')\n",
    "plt.title('Check for Linearity:\\n Actual Vs Predicted value')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Real')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Linearity --  RIGE\n",
    "X_test = full_pipeline.transform(test_set)\n",
    "y_test = test_set['charges']\n",
    "\n",
    "grid_search_lasso.fit(X_train,y_train)\n",
    "y_pred_lasso = grid_search_lasso.predict(X_test)\n",
    "\n",
    "grid_search_ridge.fit(X_train,y_train)\n",
    "y_pred_ridge = grid_search_ridge.predict(X_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.scatterplot(y_test,y_pred_ridge,color='g',alpha=0.4)\n",
    "sns.scatterplot(y_test,y_pred_lasso,color='r',alpha=0.4)\n",
    "\n",
    "plt.title('Check for Linearity:\\n Actual Vs Predicted value')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Real')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LINEAR REGRESSION\")\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "# The mean squared error\n",
    "print(' Mean squared error: %.3f'%  np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(' Coefficient of determination (R^2): %.3f'% r2_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\nRIDGE REGRESSION\")\n",
    "print(grid_search_ridge.best_params_)\n",
    "y_pred = grid_search_ridge.predict(X_test)\n",
    "# The mean squared error\n",
    "print(' Mean squared error: %.3f'%  np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(' Coefficient of determination (R^2): %.3f'% r2_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\nLASSO REGRESSION\")\n",
    "print(grid_search_lasso.best_params_)\n",
    "y_pred = grid_search_lasso.predict(X_test)\n",
    "# The mean squared error\n",
    "print(' Mean squared error: %.3f'%  np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(' Coefficient of determination (R^2): %.3f'% r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICE: Create and evalualuate a regression model for **THIS** dataset\n",
    "\n",
    "Remember the ML pipeline: \n",
    "\n",
    "#### ML Pipeline\n",
    "* Setting up the environment and data import\n",
    "* Understanding the data\n",
    "* Exploratory Data Analysis\n",
    "* Linear Regression Model\n",
    "* Preparation and splitting the data\n",
    "* Train and Test the Model\n",
    "* Train and Test New Model\n",
    "* Compare the models\n",
    "* Model Performance\n",
    "* Applying on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "housing = pd.read_csv('./dataset/housing-snapshot/train_set.csv',index_col=0)\n",
    "#housing['charges'] = np.log(housing['Price'])\n",
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.fillna(housing.mean())\n",
    "housing = housing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "numerical_features = list(housing_num)\n",
    "categorical_features = list(housing.select_dtypes(include=[object]))\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.drop(['Address','Regionname','Postcode','SellerG','Date'],axis=1)\n",
    "housing = pd.get_dummies(housing,columns= ['Type','Method','CouncilArea','Suburb'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.fillna(housing.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = housing.drop(\"Price\", axis=1) # drop labels for training set\n",
    "y_train = housing[\"Price\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X_train.values)\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = X_train.columns.tolist()\n",
    "X_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression\n",
    "regr_ridge = linear_model.Ridge(alpha=.3) # Create a Ridge regressor\n",
    "regr_ridge.fit(X_train, y_train)  # Perform the fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_coefs(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Ridge model:\", pretty_print_coefs(regr_ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso Regression\n",
    "regr_lasso = linear_model.Lasso(alpha=.3,tol=0.001) # Create a Ridge regressor\n",
    "regr_lasso.fit(X_train, y_train)  # Perform the fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Lasso model:\", pretty_print_coefs(regr_lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# load data\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Use L2 penalty\n",
    "estimator = RidgeCV(cv=5, normalize = True)\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(estimator, threshold=0.25, prefit=False, norm_order=2, max_features=12)\n",
    "\n",
    "sfm.fit(X, y)\n",
    "\n",
    "feature_idx = sfm.get_support()\n",
    "feature_name = X.columns[feature_idx]\n",
    "feature_name\n",
    "\n",
    "# n_features = sfm.transform(X).shape[1]\n",
    "# n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Ridge model:\", pretty_print_coefs(regr_lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_features = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'BuildingArea',\n",
    "       'YearBuilt', 'Lattitude', 'Longtitude', 'Type_u', 'CouncilArea_Bayside',\n",
    "       'CouncilArea_Boroondara']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# load data\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Use L1 penalty\n",
    "estimator = LassoCV(cv=5, normalize = True)\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(estimator, threshold=0.25, prefit=False, norm_order=1, max_features=12)\n",
    "\n",
    "sfm.fit(X, y)\n",
    "\n",
    "feature_idx = sfm.get_support()\n",
    "feature_name = X.columns[feature_idx]\n",
    "feature_name\n",
    "\n",
    "# n_features = sfm.transform(X).shape[1]\n",
    "# n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_features = ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude',\n",
    "       'Type_t', 'Type_u', 'CouncilArea_Boroondara', 'CouncilArea_Brimbank',\n",
    "       'CouncilArea_Wyndham', 'Suburb_Brighton']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Foward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_regression(X, y,initial_list=[],threshold_in=0.01,threshold_out = 0.05,verbose=False):\n",
    "    included=list(X.columns)\n",
    "    while True:\n",
    "        changed=False\n",
    "        model = sm.OLS(list(y), sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop  with p-value '.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "ols_features = backward_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_features = ols_features[0:12]\n",
    "ols_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ols = X_train[ols_features]\n",
    "X_train_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ridge = X_train[ridge_features]\n",
    "X_train_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lasso = X_train[lasso_features]\n",
    "X_train_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('./dataset/housing-snapshot/test_set.csv',index_col=0)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.fillna(X_test.mean())\n",
    "X_test = X_test.drop(['Address','Regionname','Postcode','SellerG','Date'],axis=1)\n",
    "X_test = pd.get_dummies(X_test,columns= ['Type','Method','CouncilArea','Suburb'],drop_first=True)\n",
    "\n",
    "X_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing columns in the training test\n",
    "missing_cols = set( X_train.columns ) - set( X_test.columns )\n",
    "print(missing_cols)\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    X_test[c] = 0\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "X_test = X_test[X_train.columns]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sc_X.transform(X_test.values)\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = X_test.columns.tolist()\n",
    "X_test = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ols = X_test[ols_features]\n",
    "X_test_ridge = X_test[ridge_features]\n",
    "X_test_lasso = X_test[lasso_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineal Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS CON 12 features diferentes (SM,Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_ols, y_train)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_predict = lin_reg.predict(X_test_ols)\n",
    "ols_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(ols_predict)\n",
    "df_output = df_output.reset_index()\n",
    "df_output.columns = ['index','Price']\n",
    "\n",
    "df_output.to_csv('ols.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "cross_val_evaluation(lin_reg,X_train_ols,y_train,'Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = linear_model.Ridge(alpha=.3,normalize = True) # Create a Ridge regressor\n",
    "cross_val_evaluation(ridge_reg,X_train_ridge,y_train,'Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = linear_model.Lasso(alpha=.3, normalize = True) # Create a Ridge regressor\n",
    "cross_val_evaluation(lasso_reg,X_train_lasso,y_train,'Lasso Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'alpha': np.logspace(-2, 5, 100)}]\n",
    "\n",
    "ridge_reg = linear_model.Ridge(normalize = True) # Create a Ridge regressor\n",
    "grid_search_ridge = GridSearchCV(ridge_reg, param_grid, cv=5,scoring=\"neg_mean_squared_error\",\n",
    "                                 return_train_score=True, n_jobs=-1)\n",
    "\n",
    "grid_search_ridge.fit(X_train_ridge, y_train)\n",
    "print('Best Score: ', grid_search_ridge.best_score_)\n",
    "print('Best Params: ', grid_search_ridge.best_params_)\n",
    "pd.DataFrame(grid_search_ridge.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range= np.logspace(-2, 5, 20)\n",
    "\n",
    "train_scores, valid_scores = validation_curve(ridge_reg, X_train_ridge, y_train, param_name= \"alpha\",\n",
    "                                              param_range= param_range,\n",
    "                                              cv=3)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.title(\"Validation Curve with Ridge Regression\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = linear_model.Ridge(alpha=0.01,normalize = True)\n",
    "ridge_reg.fit(X_train_ridge, y_train)\n",
    "ridge_reg = ridge_reg.predict(X_test_ridge)\n",
    "\n",
    "\n",
    "df_output = pd.DataFrame(ridge_reg)\n",
    "df_output = df_output.reset_index()\n",
    "df_output.columns = ['index','Price']\n",
    "df_output.to_csv('ridge.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = linear_model.Lasso(tol=0.01, normalize = True) # Create a Ridge regressor\n",
    "param_grid = [{'alpha': np.logspace(-2, 5, 1000)}]\n",
    "\n",
    "grid_search_lasso = GridSearchCV(lasso_reg, param_grid, cv=5,scoring=\"neg_root_mean_squared_error\",\n",
    "                                 return_train_score=True, n_jobs=-1)\n",
    "grid_search_lasso.fit(X_train_lasso, y_train)\n",
    "print('Best Score: ', grid_search_lasso.best_score_)\n",
    "print('Best Params: ', grid_search_lasso.best_params_)\n",
    "\n",
    "pd.DataFrame(grid_search_lasso.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same with lasso model\n",
    "param_range= np.logspace(-2, 5, 20)\n",
    "\n",
    "train_scores, valid_scores = validation_curve(lasso_reg, X_train_lasso, y_train, param_name= \"alpha\",\n",
    "                                              param_range= param_range,\n",
    "                                              cv=3)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(valid_scores, axis=1)\n",
    "test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.title(\"Validation Curve with Ridge Regression\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = linear_model.Lasso(tol=0.01,alpha=5.317,normalize = True)\n",
    "lasso_reg.fit(X_train_lasso, y_train)\n",
    "lasso_reg = lasso_reg.predict(X_test_lasso)\n",
    "\n",
    "df_output = pd.DataFrame(lasso_reg)\n",
    "df_output = df_output.reset_index()\n",
    "df_output.columns = ['index','Price']\n",
    "df_output.to_csv('laso.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION: What happens if in the problem of House Prediction Price instead of estimating the price, we estimate the log of the price?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
